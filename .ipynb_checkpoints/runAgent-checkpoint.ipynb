{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import win32gui\n",
    "import numpy as np\n",
    "\n",
    "import mss\n",
    "import mss.tools\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from cloningCNN import CloningCNN\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from pynput.keyboard import Key, Controller\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RL environment for the game window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, windowName):\n",
    "        self.gameBbox = self.getWindowBbox(windowName)\n",
    "        self.capper = mss.mss()\n",
    "        self.capNum = 0\n",
    "        self.downsamplingMethod = Image.NEAREST\n",
    "        \n",
    "        self.captureWindow = None\n",
    "        self.prevGoldPixels = None\n",
    "        self.prevLvlPixels = None\n",
    "        \n",
    "        self.keyboard = Controller()\n",
    "        self.keyCodes = {0:Key.up, 1:Key.right, 2:Key.down, 3:Key.left}\n",
    "        \n",
    "    def getWindowBbox(self, windowName):\n",
    "        gameWindow = win32gui.FindWindow(None, windowName)\n",
    "        if gameWindow == 0:\n",
    "            print(\"Could not find game window for \\\"\" + windowName + \"\\\". Exiting.\")\n",
    "            exit()\n",
    "#         win32gui.SetForegroundWindow(gameWindow) # this doesn't work in jupyter :(\n",
    "\n",
    "        bbox = list(win32gui.GetWindowRect(gameWindow))\n",
    "        # fix idiosyncrasies of win32gui window rect acquisition\n",
    "        bbox[0] += 8\n",
    "        bbox[1] += 31\n",
    "        width = bbox[2]-bbox[0]\n",
    "        height = bbox[3]-bbox[1]\n",
    "        # return a 'monitor' object in the format that the mss library wants\n",
    "        return {'top': bbox[1], 'left': bbox[0], 'width': width-8, 'height': height-8}\n",
    "    \n",
    "    def quickRestart(self):\n",
    "        for key in [Key.esc, Key.up, Key.up, Key.up, Key.right]:\n",
    "            self.keyboard.press(key)\n",
    "            self.keyboard.release(key)\n",
    "            time.sleep(0.1)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.quickRestart()\n",
    "        initScreen, initGoldPixels, initLvlPixels = self.getScreenInfo()\n",
    "        self.prevGoldPixels = initGoldPixels\n",
    "        self.prevLvlPixels = initLvlPixels\n",
    "        self.captureWindow = [initScreen for _ in range(4)]\n",
    "        \n",
    "        return self.captureWindow\n",
    "    \n",
    "    def getState(self):\n",
    "        return self.captureWindow\n",
    "    \n",
    "    def getScreenInfo(self):\n",
    "        # capture screen and downscale to 180x180\n",
    "        self.capNum += 1\n",
    "        screenCap = self.capper.grab(self.gameBbox)\n",
    "        img = Image.frombytes('RGB', screenCap.size, screenCap.bgra, 'raw', 'BGRX')\n",
    "        padded = ImageOps.expand(img, (30,38,26,18))\n",
    "        resized = padded.resize((360,360), self.downsamplingMethod)\n",
    "        final = resized.resize((180,180), self.downsamplingMethod)\n",
    "        \n",
    "        # check screen for reward indicators\n",
    "        goldPixels = np.array(final.crop((158,11,170,17))) == 255 # white pixels of gold meter in top right\n",
    "        goldPixels = goldPixels.astype(int)\n",
    "        \n",
    "        lvlPixels = np.array(final.crop((170,170,180,180))) == 255\n",
    "        lvlPixels = lvlPixels.astype(int)             \n",
    "        \n",
    "        return (final, goldPixels, lvlPixels)\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # carry out action\n",
    "        keyPress = self.keyCodes[action]\n",
    "        self.keyboard.press(keyPress)\n",
    "        self.keyboard.release(keyPress)\n",
    "        \n",
    "        # wait, then get new game capture\n",
    "        time.sleep(0.32)\n",
    "        screen, goldPixels, lvlPixels = self.getScreenInfo()\n",
    "        \n",
    "        # check if got reward\n",
    "        if np.sum(np.abs(goldPixels - self.prevGoldPixels)) != 0: # gold value changed\n",
    "            reward += 10\n",
    "        if np.sum(np.abs(lvlPixels - self.prevLvlPixels)) != 0: # beat level 1\n",
    "            reward += 10000\n",
    "            done = True\n",
    "            \n",
    "        # update state variables\n",
    "        self.prevGoldPixels = goldPixels\n",
    "        self.prevLvlPixels = lvlPixels\n",
    "        self.captureWindow.pop()\n",
    "        self.captureWindow.insert(0, screen)\n",
    "        \n",
    "        return (self.captureWindow, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloningAgent:\n",
    "    def __init__(self, paramsFile, gamma=0.99):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            print(\"using cuda\")\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        self.cloningCNN = CloningCNN('original', dropout=0.5, inChannels=12)\n",
    "        self.cloningCNN.load_state_dict(torch.load(paramsFile))\n",
    "#         self.cloningCNN.eval()\n",
    "#         self.cloningCNN = self.cloningCNN.to(device=self.device)\n",
    "        self.normTransform = self.getNormTransform()\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.optimizer = optim.Adam(self.cloningCNN.parameters(), lr=1e-3)\n",
    "        \n",
    "    def getNormTransform(self):\n",
    "        meanImage = np.load(\"data/stats/dsetMean.npy\")\n",
    "        stdImage = np.load(\"data/stats/dsetStd.npy\")\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.array(np.round(meanImage), dtype=np.uint8))\n",
    "        plt.title(\"Mean image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.array(np.round(stdImage), dtype=np.uint8))\n",
    "        plt.title(\"Std image\")\n",
    "        # get channel means formatted like tensors\n",
    "        channelMeans = meanImage.transpose((2,0,1)).mean(axis=(1,2)) / 255\n",
    "        channelStds = meanImage.transpose((2,0,1)).std(axis=(1,2)) / 255\n",
    "\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(channelMeans, channelStds)\n",
    "        ])\n",
    "        \n",
    "    def getActionScores(self, state):\n",
    "        tensors = [self.normTransform(cap).unsqueeze(0) for cap in state]\n",
    "        x = torch.cat((tensors[0],tensors[1],tensors[2],tensors[3]), dim=1)\n",
    "#         x = x.to(device=self.device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "        actionScores = self.cloningCNN(x)   #.data.cpu().numpy()[0]\n",
    "        return actionScores\n",
    "\n",
    "    def sampleAction(self, state):\n",
    "        scores = self.getActionScores(state)\n",
    "        print(scores)\n",
    "        probs = F.softmax(scores, dim=1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def finishEpisode(self):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        rewards = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            rewards.insert(0, R)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + self.eps)\n",
    "        for log_prob, reward in zip(self.saved_log_probs, rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowName = \"Crypt of the NecroDancer\"\n",
    "env = GameEnv(windowName)\n",
    "agent = CloningAgent(\"models/cloningCNN_val71_05_31_T20_35.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "log_interval = 1\n",
    "\n",
    "print(\"Move the windows taskbar!\")\n",
    "time.sleep(4) # activate the game window during this\n",
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    t, timeSinceReward = 0, 0\n",
    "    while timeSinceReward < 30:  # Don't infinite loop while learning\n",
    "        action = agent.sampleAction(state)\n",
    "        state, reward, done = env.step(action)\n",
    "        if reward != 0:\n",
    "            timeSinceReward = 0\n",
    "            print(\"Got reward {}.\".format(reward))\n",
    "        agent.rewards.append(reward)\n",
    "        aborted = win32gui.GetWindowText(win32gui.GetForegroundWindow()) != windowName\n",
    "        if aborted or done:\n",
    "            break\n",
    "        timeSinceReward += 1\n",
    "        t += 1\n",
    "    if aborted:\n",
    "        print(\"Game window no longer active. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    print('finishing episode...')\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    agent.finishEpisode()\n",
    "    \n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model greedily after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Move the windows taskbar!\")\n",
    "with torch.no_grad():\n",
    "    time.sleep(4) # activate the game window during this\n",
    "    state = env.reset()\n",
    "    while win32gui.GetWindowText(win32gui.GetForegroundWindow()) == windowName: # exit on window unfocus\n",
    "        actionScores = agent.getActionScores(state)\n",
    "        print(actionScores)\n",
    "        action = np.argmax(actionScores.data.numpy()[0])\n",
    "        state, reward, done = env.step(action)\n",
    "        if reward != 0:\n",
    "            print(\"Got reward {}.\".format(reward))\n",
    "            time.sleep(1)\n",
    "        \n",
    "print(\"Game window no longer active. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
