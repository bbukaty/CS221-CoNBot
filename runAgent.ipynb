{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import win32gui\n",
    "import numpy as np\n",
    "\n",
    "import mss\n",
    "import mss.tools\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from cloningCNN import CloningCNN, ResNet\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from pynput.keyboard import Key, Controller\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RL environment for the game window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, windowName, inputInterval, usingResNet=False):\n",
    "        self.gameBbox = self.getWindowBbox(windowName)\n",
    "        self.capper = mss.mss()\n",
    "        self.capNum = 0\n",
    "        self.downsamplingMethod = Image.NEAREST\n",
    "        self.usingResNet = usingResNet\n",
    "        \n",
    "        self.captureWindow = None\n",
    "        self.prevGoldPixels = None\n",
    "        \n",
    "        self.keyboard = Controller()\n",
    "        self.keyCodes = {0:Key.up, 1:Key.right, 2:Key.down, 3:Key.left}\n",
    "        self.inputInterval = inputInterval\n",
    "        \n",
    "    def getWindowBbox(self, windowName):\n",
    "        gameWindow = win32gui.FindWindow(None, windowName)\n",
    "        if gameWindow == 0:\n",
    "            print(\"Could not find game window for \\\"\" + windowName + \"\\\". Exiting.\")\n",
    "            exit()\n",
    "        \n",
    "        bbox = list(win32gui.GetWindowRect(gameWindow))\n",
    "        # fix idiosyncrasies of win32gui window rect acquisition\n",
    "        bbox[0] += 8\n",
    "        bbox[1] += 31\n",
    "        width = bbox[2]-bbox[0]\n",
    "        height = bbox[3]-bbox[1]\n",
    "        # return a 'monitor' object in the format that the mss library wants\n",
    "        return {'top': bbox[1], 'left': bbox[0], 'width': width-8, 'height': height-8}\n",
    "    \n",
    "    def quickRestart(self):\n",
    "        for key in [Key.esc, Key.up, Key.up, Key.up, Key.right]:\n",
    "            self.keyboard.press(key)\n",
    "            self.keyboard.release(key)\n",
    "            time.sleep(0.1)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.quickRestart()\n",
    "        initScreen, initGoldPixels, lvlDone, heartPixels = self.getScreenInfo()\n",
    "        self.prevGoldPixels = initGoldPixels\n",
    "        self.prevHeartPixels = heartPixels\n",
    "        self.captureWindow = [initScreen for _ in range(4)]\n",
    "        \n",
    "        return self.captureWindow\n",
    "    \n",
    "    def getScreenInfo(self):\n",
    "        # capture screen and downscale to 180x180\n",
    "        self.capNum += 1\n",
    "        screenCap = self.capper.grab(self.gameBbox)\n",
    "        img = Image.frombytes('RGB', screenCap.size, screenCap.bgra, 'raw', 'BGRX')\n",
    "        heartPixels = []\n",
    "        if self.usingResNet:\n",
    "            final = img.resize((224,224), self.downsamplingMethod)\n",
    "            # check for reward indicators\n",
    "            goldPixels = np.array(final.crop((201,7,218,15))) == 255 # resnet version\n",
    "            lvlPixels = np.array(final.crop((219,219,224,224))) == 255 # resnet version\n",
    "            for pixelCoord in [153,158,170,175]: # resnet version\n",
    "                heartPixels.append(np.array(final.crop((pixelCoord,10,pixelCoord+1,11))).sum())\n",
    "            print(\"WARNING: LEVEL INDICATOR BROKEN FOR RESNET\")\n",
    "        else:\n",
    "            padded = ImageOps.expand(img, (30,38,26,18))\n",
    "            resized = padded.resize((360,360), self.downsamplingMethod)\n",
    "            final = resized.resize((180,180), self.downsamplingMethod)\n",
    "            # check for reward indicators\n",
    "            goldPixels = np.array(final.crop((158,11,170,17))) == 255 # white pixels of gold meter in top right\n",
    "            levelDone = np.array(final.crop((173,174,174,176))).sum() != 1530\n",
    "            for pixelCoord in [121,125,134,138]:\n",
    "                heartPixels.append(np.array(final.crop((pixelCoord,12,pixelCoord+1,13))).sum())\n",
    "\n",
    "        return (final, goldPixels.astype(int), levelDone, heartPixels)\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # carry out action\n",
    "        keyPress = self.keyCodes[action]\n",
    "        self.keyboard.press(keyPress)\n",
    "        self.keyboard.release(keyPress)\n",
    "        \n",
    "        # wait, then get new game capture\n",
    "        time.sleep(self.inputInterval)\n",
    "        screen, goldPixels, lvlDone, heartPixels = self.getScreenInfo()\n",
    "        \n",
    "        if lvlDone: # wait a bit then recheck the screen, helps prevent false level finishes\n",
    "            time.sleep(1)\n",
    "            screen, goldPixels, lvlDone, heartPixels = self.getScreenInfo()\n",
    "             \n",
    "        # check if got reward\n",
    "        if np.sum(np.abs(goldPixels - self.prevGoldPixels)) != 0: # gold value changed\n",
    "            reward += 10\n",
    "        if lvlDone: # beat level 1\n",
    "            reward += 400\n",
    "            done = True\n",
    "            \n",
    "        # check for damage or dead\n",
    "        for heartIndex, heartPixel in enumerate(heartPixels):\n",
    "            if self.prevHeartPixels[heartIndex] != 115 and heartPixel == 115:\n",
    "                reward -= 10\n",
    "#                 print (\"lost heart {}. penalty applied.\".format(heartIndex))\n",
    "        if heartPixels[0] == 115:\n",
    "            done = True\n",
    "            \n",
    "        # update state variables\n",
    "        self.prevGoldPixels = goldPixels\n",
    "        self.prevHeartPixels = heartPixels\n",
    "        self.captureWindow.pop()\n",
    "        self.captureWindow.insert(0, screen)\n",
    "        \n",
    "        return (self.captureWindow, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloningAgent:\n",
    "    def __init__(self, model, normTransform, gamma=0.99):\n",
    "        if torch.cuda.is_available() :\n",
    "            self.device = torch.device('cuda')\n",
    "            print(\"using cuda\")\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        self.model = model.to(device=self.device, dtype=torch.float32)\n",
    "        model.eval()\n",
    "        self.normTransform = normTransform\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        \n",
    "    def getActionScores(self, state, verbose=False):\n",
    "        tensors = [self.normTransform(cap).unsqueeze(0) for cap in state]\n",
    "        x = torch.cat((tensors[0],tensors[1],tensors[2],tensors[3]), dim=1)\n",
    "        x = x.to(device=self.device, dtype=torch.float32)# move to device, e.g. GPU\n",
    "        actionScores = self.model(x)   #.data.cpu().numpy()[0]\n",
    "        if verbose:\n",
    "            print(actionScores)\n",
    "        return actionScores\n",
    "\n",
    "    def sampleAction(self, state, verbose=False):\n",
    "        scores = self.getActionScores(state, verbose)\n",
    "        probs = F.softmax(scores, dim=1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def finishEpisode(self):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        rewards = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            rewards.insert(0, R)\n",
    "        rewards = torch.tensor(rewards).to(device=self.device, dtype=torch.float32)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + self.eps)\n",
    "        for log_prob, reward in zip(self.saved_log_probs, rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize agent and environment\n",
    "Load a model for the agent to use and its cached normalization statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usingResNet = False\n",
    "\n",
    "if usingResNet:\n",
    "    model = ResNet()\n",
    "    model.load_state_dict(torch.load(\"models/ResnetQuadsVal80.pt\"))\n",
    "else:\n",
    "    model = CloningCNN('deeper', dropout=0.5, inChannels=12)\n",
    "#     model.load_state_dict(torch.load(\"models/DeeperQuadsVal79.pt\"))\n",
    "#     model.load_state_dict(torch.load(\"models/PG_deeperquads_pre_2/300.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if usingResNet:\n",
    "    meanImage = np.load(\"data/stats/dsetMean_ResNet.npy\")\n",
    "    stdImage = np.load(\"data/stats/dsetStd_ResNet.npy\")\n",
    "else:\n",
    "    meanImage = np.load(\"data/stats/dsetMean.npy\")\n",
    "    stdImage = np.load(\"data/stats/dsetStd.npy\")\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.array(np.round(meanImage), dtype=np.uint8))\n",
    "plt.title(\"Mean image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.array(np.round(stdImage), dtype=np.uint8))\n",
    "plt.title(\"Std image\")\n",
    "# get channel means formatted like tensors\n",
    "channelMeans = meanImage.transpose((2,0,1)).mean(axis=(1,2)) / 255\n",
    "channelStds = meanImage.transpose((2,0,1)).std(axis=(1,2)) / 255\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(channelMeans, channelStds)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowName = \"Crypt of the NecroDancer\"\n",
    "\n",
    "env = GameEnv(windowName, 0.36, usingResNet)\n",
    "agent = CloningAgent(model, transform, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using policy gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 20\n",
    "log_interval = 50\n",
    "rewardsOverTime = []\n",
    "modelsFolder = \"models/PG_deeperquads_nopre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Move the windows taskbar!\")\n",
    "time.sleep(4) # activate the game window during this\n",
    "for i_episode in range(800):\n",
    "    state = env.reset()\n",
    "    t, timeSinceReward, totalReward = 0, 0, 0\n",
    "    while timeSinceReward < 50:  # Don't infinite loop while learning\n",
    "        action = agent.sampleAction(state, verbose=(t%100==0))\n",
    "        state, reward, done = env.step(action)\n",
    "        if reward != 0:\n",
    "            timeSinceReward = 0\n",
    "            totalReward += reward\n",
    "#             print(\"Got reward {}.\".format(reward))\n",
    "        agent.rewards.append(reward)\n",
    "        aborted = win32gui.GetWindowText(win32gui.GetForegroundWindow()) != windowName\n",
    "        if aborted or done:\n",
    "            break\n",
    "        timeSinceReward += 1\n",
    "        t += 1\n",
    "    if aborted:\n",
    "        print(\"Game window no longer active. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    print('finishing episode...')\n",
    "    running_reward = running_reward * 0.95 + totalReward * 0.05\n",
    "    agent.finishEpisode()\n",
    "    rewardsOverTime.append(totalReward)\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:5d}\\tAverage reward: {:.2f}'.format(\n",
    "            i_episode, totalReward, running_reward))\n",
    "    if i_episode % log_interval == 0:\n",
    "        torch.save(agent.model.state_dict(), \"{}/{}.pt\".format(modelsFolder, i_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/rewards.txt\".format(modelsFolder), \"w+\") as rewardsFile:\n",
    "    for reward in rewardsOverTime:\n",
    "        rewardsFile.write(\"{}\\n\".format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rewardsOverTime)\n",
    "# plt.axis([0, 135, -40, 200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avgs = []\n",
    "N = 25\n",
    "for i in range(N):\n",
    "    avgs.append(0)\n",
    "for i in range(N, len(rewardsOverTime)):\n",
    "    avgs.append(sum(rewardsOverTime[i-N:i])/N)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(avgs)\n",
    "plt.plot(rewardsOverTime)\n",
    "# plt.axis([0, 120, -40, 200])\n",
    "plt.savefig('demos/deeperquads_nopre.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model greedily after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Move the windows taskbar!\")\n",
    "with torch.no_grad():\n",
    "    time.sleep(4) # activate the game window during this\n",
    "    state = env.reset()\n",
    "    while win32gui.GetWindowText(win32gui.GetForegroundWindow()) == windowName: # exit on window unfocus\n",
    "        actionScores = agent.getActionScores(state)\n",
    "#         print(actionScores)\n",
    "        action = np.argmax(actionScores.data.cpu().numpy()[0])\n",
    "        state, reward, done = env.step(action)\n",
    "#         time.sleep(0.4)\n",
    "        if reward != 0:\n",
    "            print(\"Got reward {}.\".format(reward))\n",
    "        \n",
    "print(\"Game window no longer active. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
